<p align="center">
  <img src="docs/landing_page.png" alt="DRISHTI - Unlock the Language of Satellite Imagery" width="800"/>
</p>

<h1 align="center">DRISHTI</h1>
<h3 align="center">Deep Remote-sensing Intelligence for Semantic Hybrid Text-Image Understanding</h3>

<p align="center">
  <a href="https://www.akshadrishti.space/"><img src="https://img.shields.io/badge/ğŸŒ_Live_Demo-akshadrishti.space-blue?style=for-the-badge" alt="Live Demo"/></a>
  <a href="docs/DRISHTI_REPORT_FINAL.pdf"><img src="https://img.shields.io/badge/ğŸ“„_Paper-PDF-red?style=for-the-badge" alt="Paper"/></a>
  <a href="#license"><img src="https://img.shields.io/badge/License-Apache_2.0-green?style=for-the-badge" alt="License"/></a>
</p>

<p align="center">
  <b>ğŸ† 4th Place â€” ISRO Problem Statement, Inter-IIT Tech Meet 14.0</b><br/>
  <b>ğŸ¥‡ Overall Champions â€” IIT Kharagpur</b>
</p>

---

## Abstract

The proliferation of satellite constellations and high-resolution aerial platforms has generated unprecedented volumes of Remote Sensing (RS) imagery, yet effective natural language interaction with such data remains a significant challenge. **DRISHTI** is a unified Vision-Language Model (VLM) framework enabling intuitive natural language interaction with RS imagery across varied resolutions, sensor modalities, and downstream tasks.

DRISHTI addresses three critical gaps in existing RS-VLM research:
1. **DRISHTI-GCV Dataset** â€” A large-scale, difficulty-aware dataset (~180K samples) spanning Grounding, Captioning, and VQA
2. **Two-Stage Curriculum Learning** â€” LoRA-tuned backbone + DPO alignment for hallucination reduction
3. **Numeric Reasoning Pipeline** â€” SAM3-based module with pyramidal tiling for accurate counting and area estimation

> **Key Results**: +40% BERT-BLEU on captioning, +21% VQA accuracy over GeoChat, and state-of-the-art counting performance.

---

## ğŸ¯ Problem Statement

> *"Is the picture really worth a thousand words?"*

The challenge, posed by **ISRO Space Applications Centre** at Inter-IIT TechMeet 14, is to design a functional prototype that empowers non-expert users to interpret and analyze satellite imagery using **natural language**.

### Required Capabilities

| Task | Description |
|------|-------------|
| **Image Captioning** | Generate descriptive captions for RS scenes |
| **Visual Question Answering** | Answer semantic, binary, and numeric questions |
| **Visual Grounding** | Localize objects based on textual queries |

### Unique Challenges in Remote Sensing

- **Multi-resolution data**: From sub-meter optical to coarse multispectral products
- **Dense object layouts**: Thousands of arbitrarily oriented, small objects
- **Multi-modal imagery**: RGB, SAR, thermal infrared, and multispectral sensors
- **Quantitative reasoning**: Object counting, size estimation, spatial relationships

See the full problem statement: [ISRO_M3_TechMeet14.pdf](docs/ISRO_M3_TechMeet14.pdf)

---

## ğŸš€ Key Contributions

### 1. DRISHTI-GCV: Difficulty-Aware RS Dataset

| Task | Samples | Resolution Range |
|------|---------|------------------|
| Captioning (Stage I) | ~22k | 256Â²â€“512Â² |
| Captioning (Stage II) | ~20k | up to 2048Â² |
| Grounding | ~60k | 224Â²â€“2048Â² |
| VQA (Generalized) | ~30k | 256Â²â€“512Â² |
| VQA (Specialized) | ~20k | 512Â²â€“2048Â² |
| SAR (SARLANG) | ~15k | 512Â² |
| Infrared (GeoAI) | ~13k | 1024Â² |
| **Total** | **~180k** | **224Â²â€“2048Â²** |

**Source Datasets**: VRSBench, Git-10M, RSVQA-LR/HR, OPT-RSVG, RSVG/RSVG-HR, DIOR/DOTA-v2

### 2. Two-Stage Curriculum Learning

**Stage I: General RS Adaptation**
- Base: `Qwen3-VL-8B-Instruct`
- Method: LoRA fine-tuning (rank=16, Î±=32, 4-bit QLoRA)
- Data: DRISHTI-GCV generalized split

**Stage II: Task Specialization**
- Captioning: SFT on high-density scenes + DPO alignment (Î²=0.1)
- VQA: Sub-classification into Semantic/Binary/Numeric
- Grounding: AHG-Net with SAM3 pyramidal tiling

<p align="center">
  <img src="docs/training_pipeline.png" alt="Training Pipeline" width="700"/>
</p>

### 3. Adaptive Hierarchical Grounding Network (AHG-Net)

The grounding pipeline proceeds through six distinct stages:

1. **Subject-Reference Query Decomposition** â€” Separate primary target from spatial anchor
2. **Pyramidal SAM3 Segmentation** â€” Multi-scale tiling to capture small objects
3. **Centroid-based Clustering** â€” Greedy box clustering to reduce redundant candidates
4. **Subject Confidence Estimation** â€” HIGH/MEDIUM/LOW classification
5. **Chunked VLM Refinement** â€” Iterative verification of ambiguous proposals
6. **Final Spatial Reasoning** â€” Directional filtering and superlative selection

---

## ğŸ—ï¸ System Architecture

<p align="center">
  <img src="docs/architecture.png" alt="DRISHTI System Architecture" width="800"/>
</p>

### Routing Logic

| Router | Function |
|--------|----------|
| **ResNet-18 Classifier** | Visual routing â€” categorizes imagery (RGB/SAR/IR/FCC) |
| **Qwen3-VL-30B** | Query routing â€” dispatches to Captioning/Grounding/VQA pipelines |

### Inference Engine

| Component | Model | Purpose |
|-----------|-------|---------|
| Primary VLM | Qwen3-VL-8B (LoRA) | Captioning, VQA, semantic reasoning |
| Task Router | Qwen3-VL-30B-A3B | Query classification |
| Segmentation | SAM3 + Pyramidal Tiling | Counting, area estimation |
| Modality Detection | ResNet-18 | RGB/SAR/IR/FCC classification |
| FCC Synthesis | Root Polynomial Correction + 3D-LUT | False Color â†’ RGB reconstruction |

### FCC-to-RGB Reconstruction

For False Color Composite imagery, we:
1. Predict spectral identity of each channel using a dual-path classifier
2. Identify missing RGB band: `B_miss = {R, G, B} \ {Å·â‚, Å·â‚‚, Å·â‚ƒ}`
3. Reconstruct using Root Polynomial Color Correction (RPCC) for R/G, or 3D-LUT for B

---

## ğŸ“Š Experimental Results

### Captioning Performance

| Model | Generalized (BERT-BLEU) | Specialized (BERT-BLEU) |
|-------|-------------------------|-------------------------|
| Qwen3-VL-8B-Instruct | 0.7066 | 0.7199 |
| InternVL3.5-8B | 0.7876 | 0.7347 |
| GPT-4o | â€” | 0.8174 |
| **DRISHTI** | **0.8216** | **0.8493** |

### VQA Performance

| Model | Binary | Numeric | Semantic |
|-------|--------|---------|----------|
| GeoChat | 0.764 | 0.356 | â€” |
| Qwen3-VL-8B | 0.948 | 0.665 | 0.833 |
| **DRISHTI** | **0.948** | **0.702** | **0.937** |

### Counting Accuracy by Object Density

| Objects | DRISHTI (SAM3 + Pyramidal) | VLM Direct |
|---------|----------------------------|------------|
| 1â€“10 | 89% | 72% |
| 11â€“50 | 74% | 41% |
| 51â€“100 | 61% | 22% |
| 100+ | 48% | 11% |

### Ablation Study

| Configuration | Binary | Numeric | Semantic |
|---------------|--------|---------|----------|
| Base Qwen3-VL-8B | 0.684 | 0.381 | 0.703 |
| + Stage I SFT | 0.721 | 0.392 | 0.756 |
| + Stage II SFT | 0.754 | 0.401 | 0.812 |
| + Question Router | 0.768 | 0.518 | 0.835 |
| + SAM3 Numeric | 0.768 | 0.642 | 0.835 |
| + DPO Alignment | **0.779** | **0.642** | **0.860** |

---

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.11+
- Node.js 18+
- MongoDB
- Modal account (for GPU inference)

### Backend Setup

```bash
cd backend
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt  # Or: uv pip install -r requirements.txt
cp .env.example .env  # Configure API keys
uvicorn app.main:app --reload --port 8000
```

### Frontend Setup

```bash
cd frontend
npm install
npm run dev  # Starts on http://localhost:3000
```

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ backend/                    # FastAPI + LangGraph Orchestrator
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/routes/         # REST endpoints (orchestrator_routes.py)
â”‚   â”‚   â”œâ”€â”€ services/           # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ ir2rgb_service.py       # FCC â†’ RGB reconstruction
â”‚   â”‚   â”‚   â”œâ”€â”€ modality_router.py      # ResNet modality detection
â”‚   â”‚   â”‚   â””â”€â”€ modal_client.py         # Modal GPU service client
â”‚   â”‚   â””â”€â”€ orchestrator.py     # LangGraph workflow (Task routing)
â”‚   â”œâ”€â”€ modal_services/         # GPU services for Modal deployment
â”‚   â”‚   â”œâ”€â”€ sam3_agent.py       # SAM3 pyramidal counting/area
â”‚   â”‚   â”œâ”€â”€ vllm_modal_deploy.py # vLLM server
â”‚   â”‚   â”œâ”€â”€ captioning.py       # Specialized captioning model
â”‚   â”‚   â”œâ”€â”€ vqa.py              # Specialized VQA model
â”‚   â”‚   â””â”€â”€ grounding.py        # AHG-Net grounding
â”‚   â””â”€â”€ scripts/                # Utility scripts (LoRA merge)
â”œâ”€â”€ frontend/                   # React/Vite UI
â”‚   â””â”€â”€ src/
â”œâ”€â”€ docs/                       # Reports and diagrams
â”‚   â”œâ”€â”€ DRISHTI_REPORT_FINAL.pdf
â”‚   â”œâ”€â”€ ISRO_M3_TechMeet14.pdf
â”‚   â”œâ”€â”€ architecture.png
â”‚   â””â”€â”€ training_pipeline.png
â””â”€â”€ internal_data/              # SAM3 agent source & sample images
    â””â”€â”€ sam3-agent/             # Full SAM3 library
```

---

## ğŸ”§ Training Hyperparameters

| Parameter | Value |
|-----------|-------|
| Base Model | Qwen3-VL-8B-Instruct |
| LoRA Rank (r) | 16 |
| LoRA Alpha (Î±) | 32 |
| Quantization | 4-bit QLoRA |
| Effective Batch Size | 256 |
| Learning Rate | 5 Ã— 10â»âµ |
| Optimizer | AdamW-8bit |
| Epochs (Stage I) | 3 |
| Epochs (Stage II) | 2 |
| DPO Î² | 0.1 |

---

## ğŸ“– References

1. Kuckreja et al., "GeoChat: Grounded Large Vision-Language Model for Remote Sensing," CVPR 2024
2. Muhtar et al., "LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model," arXiv 2024
3. Li et al., "VRSBench: A Versatile Vision-Language Benchmark Dataset for RS Image Understanding," AAAI 2024
4. Bai et al., "Qwen3-VL Technical Report," arXiv 2025
5. Ren et al., "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks," arXiv 2024
6. Lobry et al., "RSVQA: Visual Question Answering for Remote Sensing Data," arXiv 2020
7. Liu et al., "Grounding DINO: Marrying DINO with Grounded Pre-training for Open-Set Object Detection," CVPR 2023

---

## ğŸ† Team & Acknowledgments

This project was developed by **IIT Kharagpur ISRO CV Contingent** for the **ISRO Space Applications Centre** problem statement at **Inter-IIT Tech Meet 14.0**.

### Competition Results

| Achievement | Details |
|-------------|---------|
| **Problem Statement Rank** | ğŸ… **4th Place** â€” ISRO Natural Language Satellite Imagery Analysis |
| **Overall Standing** | ğŸ¥‡ **Champions** â€” IIT Kharagpur won Inter-IIT Tech Meet 14.0 |

### Acknowledgments

We thank **ISRO Space Applications Centre** for providing this challenging problem statement that pushes the boundaries of Vision-Language Models in the remote sensing domain. Special thanks to the organizers of Inter-IIT Tech Meet 14.0 for hosting this prestigious competition.

---

## ğŸ“œ License

All model weights and code are released under the **Apache 2.0 License** unless otherwise specified.

| Component | License |
|-----------|---------|
| Qwen3-VL | Apache 2.0 |
| SAM3 | Apache 2.0 |
| ResNet (Torchvision) | BSD 3-Clause |
| Custom Code | Apache 2.0 |

---

<p align="center">
  <b>Built with equal parts hard work, questionable sleep schedules, and industrial quantities of energy drinks by the Contingent Team of ISRO CV, IIT Kharagpur â€” for ISRO GeoNLI Challenge Inter IIT Tech Meet 14.0</b>
</p>
